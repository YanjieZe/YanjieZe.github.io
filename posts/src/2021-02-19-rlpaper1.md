---
title: "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning"
date: Feb 19, 2021
---
# 零、总体模型
总体框架：2个agent（1个information seeker，1个information provider）。

每个agent：Language Understanding -> Dialogue Policy -> Language Generation

# 一、Natrual Language Understanding

sentence => intent + tag：slot

多个intent，转化为多分类问题。

特殊的intent：request，它的slot为新的类。

数据增强方面：将intent和tag拼接作为新的tag，来给slot加上。

总体的模型：一个卷积encoder，两个decoder（一个intent分类器，一个slot tagger）。

评价指标：F1 measure

# 二、Dialogue Policy Learning
训两个agent，seeker和provider。

seeker的state：偏好（目标），provider提供的信息。

provider的state：seeker要求的信息和提供的限制

reward： It assigns a positive value on successful task completion (restaurant provided matches the seeker’s goal, and all seeker’s requests are answered), a negative value otherwise, and a small negative value for each dialogue turn to favor shorter interactions. However, a seeker is penalised for each request in the goal that is not expressed, and a provider is penalised for each request that is unanswered.

## WoLF-PHC

详情见另一篇blog：Win or Learn Fast Policy Hill-Climbing(WoLF-PHC)。

# 三、Natural Language Generation

一种常用的方法：rule-based/template based generation。缺点：可扩展性和可维持性不好，不好扩展到其他领域。

比较好的方法：seq2seq。



## seq2seq

本文的NLG model基于seq2seq，输入meaning representation string，生成对应的natural language template。

例子：

**Input:** act inform <food> act inform <pricerange> act offer <name>

**Output:** <name> is a great restaurant serving <food> food and it is in the <pricerange> price range



## attention

此外，用了attention机制和LSTM。



## Modified BLEU score

一个tutorial：[bleu score](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)

由于一个meaning representation会对应多个template，比如：

**MR:** act inform <pricerange> act offer <name>

**T1:** the price range at <name> is <pricerange>

**T2:** <name> is in the <pricerange> pricerange

因此，计算meaning representation的每个可能的template的bleu score，把最高的bleu score作为结果。





