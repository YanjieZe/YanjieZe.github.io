---
title: "Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition"
date: Feb 21, 2021
---
在论文**Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition**中，作者提出了Multi-Agent Dialog Policy Learning(MADPL)

总的来说，三个关键点：actor-critic，Hybrid Value Network，in-depth experiment。

# 一、Dialog Policy
## System Policy
system的action是dialog act set的子集。一个dialog act由domain、intent、slot type、slot value组成，四维。

system在t的state由四个拼接而成：

1. user action at t
2. system action at t-1
3. the belief state
4. embedding vectors of the number of query results from DB

## User Policy

user的action与system的action类似，但是value的值被user goal填了。

user在t的state由四个拼接而成：

1. system action at t-1
2. user action at t-1
3. the goal state
4. inconsistency vector that indicates the inconsistency between the systems response and user constraint C.

此外，user policy会同时输出终止信号T。

# 二、Reward Decomposition
一方面，system和user面对的情况不一样。另一方面，他们又需要共同达成目标。因此reward被分成三个部分。

## System Reward
1. empty dialog act penalty aSt = ∅
2. late answer penalty if there is a request slot triggered but the system agent does not reply the information immediately
3. task success reward based on the user agent’s description

## User Reward
1. empty dialog act penalty aUt = ∅
2. early inform penalty if the user agent requests for information when there is still a constraint slot remained to inform
3. user goal reward whether the user agents have expressed all the constraints C and requests R

## Global Reward
1. efficiency penalty that a small negative value will be given at each dialog turn
2. sub-goal completion reward once the subtask of G in a particular domain is accomplished
3. task success reward based on
user goal G

# 三、Hybrid Value Network
$$
h_s^S = tanh(f_s^S(s^S))
$$

$$
h_s^U = tanh(f_s^U(s^U))
$$

$$
V^S(s^S) = f_S(h_s^S)
$$

$$
V^U(s^U)=f_U(h_s^U)
$$

$$
V^G(s)=f_G([h_s^S;h_s^U])
$$

# 四、Optimization
因为action space太大了，训练过程分为两步：首先用conversational corpus预训练，然后用RL提升预训练模型。

## Pretrain
用beta-weighted logistic regression进行pretrain：
$$
L(X,Y;\beta)=-[\beta \cdot Y^Tlog\sigma(X)+(I-Y)^Tlog(I-\sigma(X))]
$$

## Critic Optimization
因为传统的critic优化很不稳定，因为使用下面这些loss functions：
$$
L^S_V(\theta)=(r^S+\gamma V_{\theta^-}^S(S^{'S})-V_\theta^S(s^S))^2
$$

$$
L^U_V(\theta)=(r^U+\gamma V_{\theta^-}^U(S^{'U})-V_\theta^U(s^U))^2
$$

$$
L^G_V(\theta)=(r^G+\gamma V_{\theta^-}^G(S^{'G})-V_\theta^G(s^G))^2
$$

$$
L_V = L_V^S+L_V^U+L_V^G
$$

其中，$\theta^-$是target network的参数。

# 五、总体算法流程
<center>
<img src="../imgs/rlpaper2alg.png">
</center>

