---
title: "强化学习算法面经"
date: June 14, 2021
---
# 1 Bellman方程
详细的推导可见：[强化学习经典算法笔记(零)：贝尔曼方程的推导](https://blog.csdn.net/hhy_csdn/article/details/89105908)

<img src="../imgs/bellman_equation.png">



# 2 Value Iteration
这是一种model-based方法。

具体内容见:[这里](https://zhuanlan.zhihu.com/p/33229439)

# 3 Policy Iteration
这是一种model-based方法。

具体内容见[这里](https://zhuanlan.zhihu.com/p/34006925)

# 4 Q-Learning
这是一种model-free的算法，主要通过建立Q函数，Q=Q(s,a)，来进行action的决定。

算法：

<img src='../imgs/q_learning.png'>

# 5 sarsa
sarsa算法如下，以及与Q-learning的比较。
<img src='../imgs/sarsa.jpeg'>

# 6 sarsa与Q-learning的不同（on policy与off-policy）

从算法来看, 这就是他们两最大的不同之处了. 因为 Sarsa 是说到做到型, 所以我们也叫他 on-policy, 在线学习, 学着自己在做的事情. 而 Q learning 是说到但并不一定做到, 所以它也叫作 Off-policy, 离线学习. 而因为有了 maxQ, Q-learning 也是一个特别勇敢的算法.

Q-learning 在learn的时候，用的是Max的方法，所以学的一定是最大的。但是在choose 的时候因为epsilon而存在随机性，下一个更新的不一定是最大回报对应的action。但每次更新的回报一定是max的。

Sarsa 在learn之前用epsilon choose了action，并且确定用这个action进行learn。所以learn的不一定是最大回报的。但step的action和learn的一定是同一个。

所以说Q-learning一定学max，更激进。

# 7 MC和TD的区别
详情见 [这里](https://blog.csdn.net/qq_36013249/article/details/105868739)。

总结：TD结合了MC的sampling方法和DP的bootstrap方法，是空间复杂度和时间复杂度都最低的算法，是大型状态空间问题的唯一解决方案；
MC收敛于样本的无偏估计，TD收敛于样本的确定等价估计


# 8 Model-based和Model-free的区别
详情见[这里](https://blog.csdn.net/ppp8300885/article/details/78524235)。

# 9 Policy-based和Value-based的区别
[链接](https://www.zhihu.com/question/272223357/answer/388262439)

value-based 的典型算法是DQN，policy-based是policy gradient，结合这两种具体算法可能会更好的理解。

一、处理的action space不同：value-based适合处理的action space低维离散的,适合处理连续的action space

二、针对action的价值输出不同：value-based计算出每个action的价值,policy-based一般情况下只给出较价值较高的actions。

三、更新频率不同：value-based每个action执行都可以更新，policy-based 每个episode完成之后才能更新一次。这里可以理解成下棋的时候，value-based每下一步就学习一次，而policy-based要在一盘棋下完之后再学习。

那么，有没有一个算法结合value-based和action-based的优点呢？即又能处理高维、连续的action space，又能单步更新快速学习呢？那就是Actor-Critic，DDPG等算法了。

# 10 DDPG
链接：[link](https://blog.csdn.net/kenneth_yu/article/details/78478356)



