---
title: "Win or Learn Fast Policy Hill-Climbing(WoLF-PHC)"
date: Feb 20, 2021
---
在论文**Rational and Convergent Learning in Stochastic Games**中，对于multi-agent问题提出了一个新的算法：Win or Learn Fast Policy Hill-Climbing，简称为WoLF-PHC。

# 一、Game Theory
## Stochastic Game

$$
(n, S, A_{1...n},T,R_{1...n})
$$

$$
T:S\times A\times S\rightarrow[0,1]
$$

$$
R_i: S\times A \rightarrow \mathbb{R}
$$

$$
dicount\ factor: \gamma
$$

与MDP（Markov Decision Process）不同的地方：多个agent，下一步的state和reward由joint action决定。



博弈论概念：[Matrix Game](https://encyclopedia2.thefreedictionary.com/Matrix+Games)

两个经典的matrix game：猜硬币（match pennies），石头剪刀布（rock-paper-scissors）。前者为zero-sum SG，后者为general-sum SG。

stochastic game中的每个state可以看做是一个matrix game。

因此SG的框架既包含MDP也包含matrix game。



## Mixed Policy

agent的policy可以被其他agent利用，因此：
$$
\rho :S\rightarrow PD(A_i)
$$
 上式为一个函数，将state映射为agent's action的probability distribution。下标为agent的编号。



## Nash Equilibria

著名的博弈论理论，纳什均衡。

论文中的解释：

> A Nash equilibrium is a collection of strategies for each of the players such that each player’s strategy is a best-response to the other players’ strategies.
>
> So, no player can get a higher payoff by changing strategies given that the other players also don’t change strategies. 

理解纳什均衡：[通过几个例子理解博弈论与纳什均衡 - 笑虎的文章 - 知乎](https://zhuanlan.zhihu.com/p/25781797)



# 二、Motivation：Two Properties

**Property 1** *(Rationality) If the other players’ policies converge to stationary policies then the learning algorithm will converge to a policy that is a best-response to their policies.*

若其他agent的policy固定下来，一个agent应该有能力学习到最好的policy来应对其他agent。



**Property 2**  *(Convergence) The learner will necessarily converge to a stationary policy. This property will usually be conditioned on the other agents using an algorithm from some class of learning algorithms.*

agent的policy能够收敛。



结合这两个property，learner就可以收敛到一个最优的固定策略。



# 三、其他Reinforcement Learner的不足

**Single-Agent Learners**: 满足性质1，不总是满足性质2。

**Joint Action Learners**：与Single-Agent一样，rational but not convergent。

**Minimax-Q**：convergent but not rational。



# 四、New algorithm

## Policy Hill Climbing

<center>
<img src="../imgs/hillclimbing.png">
</center>


$$
when \ \delta =1, this\ is\ equivalent\ to \ Q-learning
$$


## WoLF Policy Hill-Climbing

**WoLF principle**: *learn quickly while losing and slowly while winning*

<center>
<img src="../imgs/wolf.png">
</center>

$$
\delta_w:winning\ learning\ rate
$$

$$
\delta_l: losing\ learning\ rate
$$

$$
\delta_w < \delta_l
$$

先计算average policy，然后计算总的Q值看看是“win” or “lose”。



# 五、Results

## Matrix Game

WoLF效果很好，可以收敛。PHC不能收敛。

## Gridworld

WoLF效果很好。

## Soccer

固定的策略是一定会被击败，因此需要mixed policy。WoLF效果比Minimax-Q好。

