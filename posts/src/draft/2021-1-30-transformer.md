---
title: "Point Transformer"
date: Feb 01, 2021
---
# 一、背景
self-attention operators分为两类：标量attention和向量attention。

> 疑问：cardinality是什么？

# 二、Point Transformer Layer
<center>
<img src="../imgs/formula14.png">
</center>

<center>
<img src="../imgs/transformer.png">
</center>