---
title: "QMIX"
date: Mar 31, 2021
---
Paper：QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning, ICML 2018.

# 一、idea
在**IQL（Independent Q Learning）**和**COMA（Counterfactual Multi-agent）**之间寻找平衡，在**VDN（Value Decomposition Network）**上做出升级。

在全局Q和单体Q上加上一个非常重要的约束，保证Q_total的有效：
$$
\frac{\partial Q_{tot}}{\partial Q_a}\geq 0, \forall a
$$
此外，每个Q_a用一个network，mix起来的时候也用一个network。



# 二、背景：Dec-POMDP

一个fully cooperative  multi-agent task可以用**Dec-POMDP**来描述。(Decentralized Partially Oberverable Markov Decision Process)
$$
G=<S,U,P,r,Z,O,n,\gamma>
$$
每轮，agents形成一个joing action:
$$
u^a \in U \rightarrow \mathbf{u} \in U^n \equiv\mathbf{U}
$$
这导致状态转移：
$$
P(s'|s,\mathbf{u}):S\times \mathbf{U} \times S \rightarrow [0,1]
$$
所有的agent共享一个reward function:
$$
r(s,\mathbf{u}): S\times \mathbf{U} \rightarrow \mathbb{R}
$$
以及discount factor：
$$
\gamma \in [0,1)
$$




考虑一个Partially Observable的场景，agent根据state和action获得observation z:
$$
O(s,a): S\times A \rightarrow Z
$$
每个agent有一个action- observation的history：
$$
\tau^a \in T\equiv(Z\times U)^*
$$
基于这个history，agent才有stochastic policy：
$$
\pi^a(u^a|\tau^a):T\times U \rightarrow [0,1]
$$
而对于joint policy，有一个joint action-value function：
$$
Q^\pi (s_t, \mathbf{u}_t) = \mathbb{E}_{s_{t+1}:\infty, \mathbb{u_{t+1}}:\infty}[R_t|s_t,\mathbb{u_t}]
$$


注意：centralized training， decentralized execution。



# 三、QMIX

忙完再看！！