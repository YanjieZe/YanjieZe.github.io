---
title: "QMIX"
date: Mar 31, 2021
---
Paper：QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning, ICML 2018.

# 一、idea
在**IQL（Independent Q Learning）**和**COMA（Counterfactual Multi-agent）**之间寻找平衡，在**VDN（Value Decomposition Network）**上做出升级。

在全局Q和单体Q上加上一个非常重要的约束，保证Q_total的有效：
$$
\frac{\partial Q_{tot}}{\partial Q_a}\geq 0, \forall a
$$
此外，每个Q_a用一个network，mix起来的时候也用一个network。



# 二、背景：Dec-POMDP

一个fully cooperative  multi-agent task可以用**Dec-POMDP**来描述。(Decentralized Partially Oberverable Markov Decision Process)
$$
G=<S,U,P,r,Z,O,n,\gamma>
$$
每轮，agents形成一个joing action:
$$
u^a \in U \rightarrow \mathbf{u} \in U^n \equiv\mathbf{U}
$$
这导致状态转移：
$$
P(s'|s,\mathbf{u}):S\times \mathbf{U} \times S \rightarrow [0,1]
$$
所有的agent共享一个reward function:
$$
r(s,\mathbf{u}): S\times \mathbf{U} \rightarrow \mathbb{R}
$$
以及discount factor：
$$
\gamma \in [0,1)
$$




考虑一个Partially Observable的场景，agent根据state和action获得observation z:
$$
O(s,a): S\times A \rightarrow Z
$$
每个agent有一个action- observation的history：
$$
\tau^a \in T\equiv(Z\times U)^*
$$
基于这个history，agent才有stochastic policy：
$$
\pi^a(u^a|\tau^a):T\times U \rightarrow [0,1]
$$
而对于joint policy，有一个joint action-value function：
$$
Q^\pi (s_t, \mathbf{u}_t) = \mathbb{E}_{s_{t+1}:\infty, \mathbb{u_{t+1}}:\infty}[R_t|s_t,\mathbb{u_t}]
$$


注意：centralized training， decentralized execution。

# 三、Value Decomposition Network
直接用每个agent的Q的和作为总体的Q。
$$
Q_{tot}(\mathbf{\tau,u})=\sum_{i=1}^n Q_i(\tau^i,u^i;\theta^i)
$$

# 四、QMIX
## （1）公式
> Key to our method is the insight that the full factorisation of VDN is not necessary in order to be able to extract decentralised policies that are fully consistent with their centralised counterpart. Instead, for consistency we only need to ensure that a global argmax performed on Qtot yields the same result as a set of individual argmax operations
performed on each Qa:
$$
\argmax_u Q_{tot}(\mathbf{\tau,u})=\begin{pmatrix} argmax_{u^1}Q_1(\tau^1,u^1)\\.\\.\\.\\argmax_{u^n}(\tau^n,u^n)\end{pmatrix}
$$
此外，还要保证：
$$
\frac{\partial Q_{tot}}{\partial Q_a}\geq 0, \forall a
$$

## （2）模型

总的框架：
<center>
<img src="../imgs/qmix.png">
</center>

对于每个agent，用一个DRQN来表示值函数，如上图（c）所示。

mixing network的权重由hypernetwork生成，利用了全局状态s（而不是观察状态），如上图（a）所示。

损失函数：
<center>
<img src="../imgs/qmix_loss.png">
</center>

## (3)Representational Complexity
对于那些action依赖于在同一轮中的其他agent的值函数，QMIX可能不能正确地进行值分解。但是QMIX至少比VDN做得好。

# 五、Two-Step Game
<center>
<img src="../imgs/two_step_game.png">
</center>
设计了一个简单的game来测试QMIX和VDN的效果。

在第一步，agent1选一个矩阵，agent2不动。

在第二步，agent1和agent2各自take action，获得global reward。

结果展示在下表中。可以看出，QMIX成功学习到了最优的策略：reward为8的情况。


<center>
<img src="../imgs/qmix_table2.png">
</center>


# 六、Ablations
做ablation主要是为了调查：
1. 额外状态信息的影响
2. 非线性mixing network的必要性
   
# Awesome Links☆
- [伏羲讲堂多智能体强化学习中的值函数分解——VDN、QMIX、QTRAN](https://zhuanlan.zhihu.com/p/203164554)
这篇文章讲的很好，倾情推荐。

- 这篇文章教你[如何搭建starcraft2环境](https://soygema.github.io/starcraftII_machine_learning/#0)
