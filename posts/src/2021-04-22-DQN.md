---
title: "Apply DQN"
date: Apr 22, 2021
---
Reference: 

[Deep Q Network(DQN)- Applying Neural Network as a functional approximation in Q-learning](https://medium.com/intro-to-artificial-intelligence/deep-q-network-dqn-applying-neural-network-as-a-functional-approximation-in-q-learning-6ffe3b0a9062)

[Reinforcement Learning w/ Keras + OpenAI: DQNs](https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c)

# 1 Basic Structure

In DQN, we make use of two separate networks with the same architecture to estimate the target and prediction Q values for the stability of the Q-learning algorithm. The result from the target model is treated as the ground truth for the prediction network. Prediction network’s weights get updated in every iteration and the target network’s weights get updated with prediction network weights after N iteration.

<center>
<img src="../imgs/dqn.png">
</center>

# 2 Pseudocode for DQN

1. Episode begins.
2. Perform action a_t from state st and observe the next state s_t+1 and reward r. Initially, the action is randomly selected. Over time, the action from the prediction network is used based on epsilon-greedy policy. In epsilon-greedy policy, an action which has the highest Q value will be considered. In a typical neural network, prediction with the highest probability will be selected.
3. Compute the reward for the action.
4. Store the current state, action, reward, and next state in replay buffer. It enables the batch training in NN.
5. s_t+1 is the new state s_t and repeat steps 2 to 4 until the batch size reaches.
Run the batch training in NN of we reach the batch size. Please note the representation of the loss function different in the implementation. You can refer [3] for getting the understanding of the code.
6. After we reach N iteration, the weights of the prediction network get copied to Target Network.
7. Episode ends.
8. Repeat steps 1 to 7 until the optimal Q value is reached.

# 3 DQN Agent Implementation
The Deep Q Network revolves around continuous learning, meaning that we don’t simply accrue a bunch of trial/training data and feed it into the model. Instead, we create training data through the trials we run and feed this information into it directly after running the trial.

## 3.1 Hyper Parameter

```python
class DQN:
    def __init__(self, env):
        self.env     = env
        self.memory  = deque(maxlen=2000)
        
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.01
```

The “memory” is a key component of DQNs: as mentioned previously, the trials are used to continuously train the model. However, rather than training on the trials as they come in, we add them to memory and train on a random sample of that memory. Why do this instead of just training on the last x trials as our “sample?”
This is because: by taking a random sample, we don’t bias our training set, and instead ideally learn about scaling all environments we would encounter equally well.

The "epsilon" is also a key component, for RL.

## 3.2 create model
```python
def create_model(self):
        model   = Sequential()
        state_shape  = self.env.observation_space.shape
        model.add(Dense(24, input_dim=state_shape[0], 
            activation="relu"))
        model.add(Dense(48, activation="relu"))
        model.add(Dense(24, activation="relu"))
        model.add(Dense(self.env.action_space.n))
        model.compile(loss="mean_squared_error",
            optimizer=Adam(lr=self.learning_rate))
        return model
```

Use this function to define two model.

```python
def __init__(self, env):
        self.env     = env
        self.memory  = deque(maxlen=2000)
        
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.01
        self.tau = .05
        self.model = self.create_model()
        # "hack" implemented by DeepMind to improve convergence
        self.target_model = self.create_model()
```

The fact that there are two separate models, one for doing predictions and one for tracking “target values” is definitely counter-intuitive. To be explicit, the role of the model (self.model) is to do the actual predictions on what action to take, and the target model (self.target_model) tracks what action we want our model to take.

## 3.3 remember
```python
def remember(self, state, action, reward, new_state, done):
        self.memory.append([state, action, reward, new_state, done])
```

## 3.4 replay
```python
def replay(self):
        batch_size = 32
        if len(self.memory) < batch_size: 
            return
        samples = random.sample(self.memory, batch_size)
        for sample in samples:
            state, action, reward, new_state, done = sample
            target = self.target_model.predict(state)
            if done:
                target[0][action] = reward
            else:
                Q_future = max(
                    self.target_model.predict(new_state)[0])
                target[0][action] = reward + Q_future * self.gamma
            self.model.fit(state, target, epochs=1, verbose=0)
```

## 3.5 target train
```python
def target_train(self):
        weights = self.model.get_weights()
        target_weights = self.target_model.get_weights()
        for i in range(len(target_weights)):
            target_weights[i] = weights[i]
        self.target_model.set_weights(target_weights)
```

## 3.6 action
```python
def act(self, state):
        self.epsilon *= self.epsilon_decay
        self.epsilon = max(self.epsilon_min, self.epsilon)
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        return np.argmax(self.model.predict(state)[0])
```

## 3.7 train agent
```python
def main():
    env     = gym.make("MountainCar-v0")
    gamma   = 0.9
    epsilon = .95
    trials  = 100
    trial_len = 500
    updateTargetNetwork = 1000
    dqn_agent = DQN(env=env)
    steps = []
    for trial in range(trials):
        cur_state = env.reset().reshape(1,2)
        for step in range(trial_len):
            action = dqn_agent.act(cur_state)
            env.render()
            new_state, reward, done, _ = env.step(action)
            reward = reward if not done else -20
            print(reward)
            new_state = new_state.reshape(1,2)
            dqn_agent.remember(cur_state, action, 
                reward, new_state, done)
            
            dqn_agent.replay()
            dqn_agent.target_train()
            cur_state = new_state
            if done:
                break
        if step >= 199:
            print("Failed to complete trial")
        else:
            print("Completed in {} trials".format(trial))
            break
```